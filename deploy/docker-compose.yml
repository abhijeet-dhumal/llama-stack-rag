version: '3.8'

services:
  # Ollama service for local LLM inference
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ../ollama_data:/root/.ollama
    environment:
      - OLLAMA_ORIGINS=*
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    # For GPU support (uncomment if you have NVIDIA GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Model initialization service
  model-init:
    image: curlimages/curl:latest
    container_name: model-init
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ../scripts/init-models.sh:/init-models.sh:ro
    command: ["/bin/sh", "/init-models.sh"]
    restart: "no"
    networks:
      - default

  # RAG Pipeline API
  rag-pipeline:
    build: 
      context: ..
      dockerfile: deploy/Dockerfile
    container_name: rag-pipeline
    ports:
      - "8000:8000"
    volumes:
      - ../chroma_db:/app/chroma_db
      - ../sample_docs:/app/sample_docs
      - ../logs:/app/logs
      - ../static:/app/static
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - VECTOR_DB_PATH=/app/chroma_db
      - LOG_LEVEL=INFO
      - ENVIRONMENT=production
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - CORS_ORIGINS=*
      - MAX_UPLOAD_SIZE=10485760  # 10MB
    depends_on:
      model-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Redis for caching (optional)
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    profiles:
      - cache

  # Nginx reverse proxy (optional)
  nginx:
    image: nginx:alpine
    container_name: nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ../ssl:/etc/nginx/ssl:ro
    depends_on:
      - rag-pipeline
    restart: unless-stopped
    profiles:
      - production

volumes:
  redis_data:
  ollama_data:

networks:
  default:
    name: rag-network 