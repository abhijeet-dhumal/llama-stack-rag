# ğŸš€ RAG Pipeline Deployment Guide

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Kubernetes/OpenShift Cluster                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                        llama-stack-rag Namespace                    â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚   Ingress/Route â”‚    â”‚  Load Balancer  â”‚    â”‚    External     â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  (External      â”‚â—„â”€â”€â–ºâ”‚   (Optional)    â”‚â—„â”€â”€â–ºâ”‚     Users       â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   Access)       â”‚    â”‚                 â”‚    â”‚                 â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚           â”‚                                                         â”‚   â”‚
â”‚  â”‚           â–¼                                                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚   â”‚
â”‚  â”‚  â”‚ RAG Pipeline    â”‚                                                â”‚   â”‚
â”‚  â”‚  â”‚ Service         â”‚                                                â”‚   â”‚
â”‚  â”‚  â”‚ (ClusterIP)     â”‚                                                â”‚   â”‚
â”‚  â”‚  â”‚ Port: 8000      â”‚                                                â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚   â”‚
â”‚  â”‚           â”‚                                                         â”‚   â”‚
â”‚  â”‚           â–¼                                                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚   â”‚
â”‚  â”‚  â”‚ RAG Pipeline    â”‚    â”‚ Ollama Service  â”‚                         â”‚   â”‚
â”‚  â”‚  â”‚ Deployment      â”‚â—„â”€â”€â–ºâ”‚ (ClusterIP)     â”‚                         â”‚   â”‚
â”‚  â”‚  â”‚                 â”‚    â”‚ Port: 11434     â”‚                         â”‚   â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚                 â”‚                         â”‚   â”‚
â”‚  â”‚  â”‚ â”‚  FastAPI    â”‚ â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                         â”‚   â”‚
â”‚  â”‚  â”‚ â”‚  Uvicorn    â”‚ â”‚    â”‚ â”‚   Ollama    â”‚ â”‚                         â”‚   â”‚
â”‚  â”‚  â”‚ â”‚  RAG Logic  â”‚ â”‚    â”‚ â”‚   LLM API   â”‚ â”‚                         â”‚   â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â”‚  (llama3.2) â”‚ â”‚                         â”‚   â”‚
â”‚  â”‚  â”‚                 â”‚    â”‚ â”‚ (nomic-emb) â”‚ â”‚                         â”‚   â”‚
â”‚  â”‚  â”‚ Resources:      â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                         â”‚   â”‚
â”‚  â”‚  â”‚ CPU: 1-2 cores  â”‚    â”‚                 â”‚                         â”‚   â”‚
â”‚  â”‚  â”‚ RAM: 2-4 GB     â”‚    â”‚ Resources:      â”‚                         â”‚   â”‚
â”‚  â”‚  â”‚ Replicas: 1-N   â”‚    â”‚ CPU: 1-2 cores  â”‚                         â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ RAM: 2-4 GB     â”‚                         â”‚   â”‚
â”‚  â”‚           â”‚             â”‚ Replicas: 1     â”‚                         â”‚   â”‚
â”‚  â”‚           â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚   â”‚
â”‚  â”‚           â”‚                       â”‚                                 â”‚   â”‚
â”‚  â”‚           â–¼                       â–¼                                 â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚   â”‚
â”‚  â”‚  â”‚ Persistent      â”‚    â”‚ Persistent      â”‚                         â”‚   â”‚
â”‚  â”‚  â”‚ Volume          â”‚    â”‚ Volume          â”‚                         â”‚   â”‚
â”‚  â”‚  â”‚ (ChromaDB)      â”‚    â”‚ (Ollama Models) â”‚                         â”‚   â”‚
â”‚  â”‚  â”‚ Size: 5Gi       â”‚    â”‚ Size: 10Gi      â”‚                         â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚   â”‚
â”‚  â”‚  â”‚ Model Init Job  â”‚                                                â”‚   â”‚
â”‚  â”‚  â”‚ (Completed)     â”‚                                                â”‚   â”‚
â”‚  â”‚  â”‚ - Downloads     â”‚                                                â”‚   â”‚
â”‚  â”‚  â”‚ - Initializes   â”‚                                                â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Deployment Options

### 1. **Kubernetes Deployment**
```bash
# Deploy all components
kubectl apply -f k8s-deployment.yaml

# Check deployment status
kubectl get pods -n llama-stack-rag
kubectl get services -n llama-stack-rag
kubectl get ingress -n llama-stack-rag
```

### 2. **OpenShift Deployment**
```bash
# Deploy components
oc apply -f k8s-deployment.yaml

# Create route for external access
oc expose service/rag-pipeline-service --port=8000 -n llama-stack-rag

# Get route URL
oc get route -n llama-stack-rag
```

### 3. **Docker Compose (Local)**
```bash
# Start services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

## ğŸŒ Access Methods

### **Option 1: Port Forwarding (Development)**
```bash
kubectl port-forward -n llama-stack-rag service/rag-pipeline-service 8000:8000
# Access: http://localhost:8000
```

### **Option 2: Ingress (Kubernetes)**
```bash
# Add to /etc/hosts
echo "127.0.0.1 rag-pipeline.local" | sudo tee -a /etc/hosts
# Access: http://rag-pipeline.local
```

### **Option 3: Route (OpenShift)**
```bash
# Get route URL
oc get route/rag-pipeline-service -n llama-stack-rag -o jsonpath='{.spec.host}'
# Access: https://rag-pipeline-service-llama-stack-rag.apps.cluster.com
```

## ğŸ“Š Scalability Features

### **Horizontal Pod Autoscaling**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: rag-pipeline-hpa
  namespace: llama-stack-rag
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: rag-pipeline-deployment
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

### **Resource Scaling Guidelines**
- **RAG Pipeline**: Scale replicas based on query load
- **Ollama Service**: Keep single replica (stateful model serving)
- **Storage**: Use dynamic provisioning for auto-scaling volumes

## ğŸ”§ Configuration

### **Environment Variables (ConfigMap)**
```yaml
OLLAMA_BASE_URL: "http://ollama-service:11434"
VECTOR_DB_PATH: "/app/chroma_db"
LOG_LEVEL: "INFO"
ENVIRONMENT: "production"
API_HOST: "0.0.0.0"
API_PORT: "8000"
```

### **Resource Requirements**
| Component      | CPU        | Memory  | Storage |
|----------------|------------|---------|---------|
| RAG Pipeline   | 1-2 cores  | 2-4 GB  | -       |
| Ollama Service | 1-2 cores  | 2-4 GB  | -       |
| ChromaDB       | -          | -       | 5 GB    |
| Ollama Models  | -          | -       | 10 GB   |

## ğŸ“‹ Deployment Checklist

- [ ] Kubernetes/OpenShift cluster ready
- [ ] Sufficient cluster resources available
- [ ] Storage classes configured
- [ ] Ingress controller installed (for K8s)
- [ ] Image registry accessible
- [ ] Namespace created
- [ ] Persistent volumes available

## ğŸ” Monitoring & Health Checks

### **Health Endpoints**
- RAG Pipeline: `http://service:8000/health`
- Ollama Service: `http://service:11434/api/tags`

### **Checking Deployment Status**
```bash
# Check all resources
kubectl get all -n llama-stack-rag

# Check pod logs
kubectl logs -f deployment/rag-pipeline-deployment -n llama-stack-rag
kubectl logs -f deployment/ollama-deployment -n llama-stack-rag

# Check persistent volumes
kubectl get pv,pvc -n llama-stack-rag
```

## ğŸš¨ Troubleshooting

### **Common Issues**
1. **Model Download Timeout**: Increase job timeout in model-init-job
2. **Storage Issues**: Check PV/PVC status and storage class
3. **Network Issues**: Verify service discovery and DNS resolution
4. **Resource Limits**: Check CPU/Memory limits and requests

### **Debug Commands**
```bash
# Describe problematic pods
kubectl describe pod <pod-name> -n llama-stack-rag

# Check events
kubectl get events -n llama-stack-rag --sort-by='.lastTimestamp'

# Check resource usage
kubectl top pods -n llama-stack-rag
```

## ğŸ“¦ Files Structure

```
deploy/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ docker-compose.yml        # Docker Compose setup
â”œâ”€â”€ docker-compose.override.yml # Local overrides
â”œâ”€â”€ k8s-deployment.yaml       # Kubernetes/OpenShift deployment
â”œâ”€â”€ Dockerfile               # Container image
â”œâ”€â”€ nginx.conf               # Nginx configuration
â””â”€â”€ run.sh                   # Deployment script
```

## ğŸ¯ Production Considerations

1. **Security**: Use non-root containers, network policies
2. **Monitoring**: Implement Prometheus/Grafana monitoring
3. **Backup**: Regular backup of persistent volumes
4. **Updates**: Rolling updates for zero-downtime deployments
5. **Load Testing**: Validate performance under expected load 