# ğŸš€ **RAG NotebookLM with LlamaStack**

> **A modern, lightweight RAG application that behaves like Google NotebookLM**  
> Built with LlamaStack orchestration and optimized for M4 MacBooks

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![LlamaStack](https://img.shields.io/badge/LlamaStack-0.0.40+-green.svg)](https://github.com/meta-llama/llama-stack)

---

## ğŸ“‹ **Table of Contents**

1. [System Architecture](#system-architecture)
2. [LlamaStack Integration](#llamastack-integration)  
3. [Data Flow & Processing](#data-flow--processing)
4. [Features](#features)
5. [Quick Start](#quick-start)
6. [Development](#development)
7. [API Documentation](#api-documentation)

---

## ğŸ›ï¸ **System Architecture**

### **High-Level Architecture Overview**

```mermaid
graph TB
    subgraph "Frontend Layer"
        UI[Streamlit UI<br/>ğŸ“± Document Upload & Chat]
        UPLOAD[ğŸ“„ File Upload Component]
        CHAT[ğŸ’¬ Chat Interface]
        SOURCES[ğŸ“š Source Management]
        AUDIO[ğŸµ Audio Player]
    end
    
    subgraph "API Gateway Layer"
        API[FastAPI Server<br/>ğŸŒ Main API Endpoints]
        WS[WebSocket Handler<br/>âš¡ Real-time Chat]
        MIDDLEWARE[ğŸ”’ CORS & Auth Middleware]
    end
    
    subgraph "LlamaStack Orchestration Hub"
        LS[ğŸ¦™ LlamaStack Server<br/>Core Orchestrator]
        ROUTER[ğŸ”€ Request Router]
        PROVIDERS[ğŸ§© Provider Manager]
    end
    
    subgraph "Document Processing Pipeline"
        INGEST[ğŸ“¥ Document Ingester<br/>PDF, DOCX, URL, MD]
        EXTRACT[ğŸ”¤ Text Extractor<br/>Clean Content Output]
        CHUNK[âœ‚ï¸ Smart Chunker<br/>Semantic Boundaries]
        EMBED[ğŸ§® Embedding Service<br/>Granite Embeddings]
    end
    
    subgraph "LlamaStack Providers"
        INF_PROV[ğŸ¤– Inference Provider<br/>Granite-3.3-8B-Instruct]
        EMB_PROV[ğŸ§® Embedding Provider<br/>Granite-Embedding-30M]
        VEC_PROV[ğŸ—„ï¸ Vector Provider<br/>SQLite-Vec]
        MEM_PROV[ğŸ§  Memory Provider<br/>Conversation Buffer]
        SAFE_PROV[ğŸ›¡ï¸ Safety Provider<br/>Granite Guardian]
    end
    
    subgraph "Storage Layer"
        VDB[(ğŸ” Vector Database<br/>SQLite-Vec)]
        META[(ğŸ“Š Metadata Store<br/>Document Info)]
        FILES[(ğŸ“ File Storage<br/>Raw Documents)]
    end
    
    subgraph "M4 Optimization Layer"
        MPS[âš¡ Metal Performance Shaders]
        ARM[ğŸ—ï¸ ARM64 Native Libraries]
        CACHE[ğŸ’¾ Intelligent Caching]
    end
    
    %% User Interactions
    UI --> API
    UPLOAD --> API
    CHAT --> WS
    
    %% API to LlamaStack
    API --> LS
    WS --> LS
    
    %% LlamaStack Internal Flow
    LS --> ROUTER
    ROUTER --> PROVIDERS
    PROVIDERS --> INF_PROV
    PROVIDERS --> EMB_PROV
    PROVIDERS --> VEC_PROV
    PROVIDERS --> MEM_PROV
    PROVIDERS --> SAFE_PROV
    
    %% Document Processing Pipeline
    API --> INGEST
    INGEST --> EXTRACT
    EXTRACT --> CHUNK
    CHUNK --> EMB_PROV
    EMB_PROV --> VEC_PROV
    VEC_PROV --> VDB
    EXTRACT --> META
    
    %% RAG Query Flow
    CHAT --> INF_PROV
    INF_PROV --> VEC_PROV
    VEC_PROV --> VDB
    VDB --> INF_PROV
    INF_PROV --> SAFE_PROV
    SAFE_PROV --> CHAT
    
    %% M4 Optimization
    INF_PROV --> MPS
    EMB_PROV --> ARM
    VEC_PROV --> CACHE
    
    %% Storage Connections
    META --> FILES
    VDB --> FILES
```

### **Microservices Architecture**

```mermaid
graph LR
    subgraph "Client Tier"
        WEB[ğŸŒ Web UI<br/>Streamlit]
        MOBILE[ğŸ“± Mobile<br/>Future]
    end
    
    subgraph "API Tier"
        GATEWAY[ğŸšª API Gateway<br/>FastAPI]
        AUTH[ğŸ” Auth Service<br/>JWT]
        RATE[â±ï¸ Rate Limiter<br/>Redis]
    end
    
    subgraph "Business Logic Tier"
        DOC[ğŸ“„ Document Service<br/>Processing Pipeline]
        CHAT[ğŸ’¬ Chat Service<br/>Q&A Logic]
        EMBED[ğŸ§® Embedding Service<br/>Vector Generation]
        SEARCH[ğŸ” Search Service<br/>Similarity Search]
    end
    
    subgraph "LlamaStack Tier"
        LLAMA[ğŸ¦™ LlamaStack<br/>Orchestrator]
        INF[ğŸ¤– Inference<br/>Granite LLM]
        VEC[ğŸ—„ï¸ Vector Store<br/>SQLite-Vec]
        MEM[ğŸ§  Memory<br/>Context]
    end
    
    subgraph "Data Tier"
        DB[(ğŸ—ƒï¸ PostgreSQL<br/>Metadata)]
        CACHE[(âš¡ Redis<br/>Cache)]
        FILES[(ğŸ“ File System<br/>Documents)]
    end
    
    WEB --> GATEWAY
    MOBILE --> GATEWAY
    GATEWAY --> AUTH
    GATEWAY --> DOC
    GATEWAY --> CHAT
    
    DOC --> EMBED
    CHAT --> SEARCH
    EMBED --> LLAMA
    SEARCH --> LLAMA
    
    LLAMA --> INF
    LLAMA --> VEC
    LLAMA --> MEM
    
    DOC --> DB
    CHAT --> CACHE
    EMBED --> FILES
```

---

## ğŸ¦™ **LlamaStack Integration**

### **Provider Configuration Matrix**

| Provider Type | Implementation | Model/Service | Configuration |
|---------------|---------------|---------------|---------------|
| **ğŸ¤– Inference** | `meta-reference` | Granite-3.3-8B-Instruct | MPS, FP16, 4K context |
| **ğŸ§® Embedding** | `meta-reference` | Granite-Embedding-30M | MPS, FP16, 512 tokens |
| **ğŸ—„ï¸ Vector Store** | `sqlite-vec` | SQLite-Vec | Local DB, 384 dims |
| **ğŸ§  Memory** | `simple` | Conversation Buffer | 8K token limit |
| **ğŸ›¡ï¸ Safety** | `meta-reference` | Granite Guardian | Content filtering |

### **LlamaStack Request Flow**

```mermaid
sequenceDiagram
    participant Client as ğŸ“± Client
    participant API as ğŸŒ FastAPI
    participant LS as ğŸ¦™ LlamaStack
    participant Inf as ğŸ¤– Inference
    participant Emb as ğŸ§® Embedding
    participant Vec as ğŸ—„ï¸ Vector Store
    participant Safe as ğŸ›¡ï¸ Safety
    
    Note over Client,Safe: Document Upload Flow
    Client->>API: POST /documents (PDF)
    API->>LS: Process Document
    LS->>Emb: Generate Embeddings
    Emb-->>LS: Vector Embeddings
    LS->>Vec: Store Vectors + Metadata
    Vec-->>LS: Storage Confirmation
    LS-->>API: Processing Complete
    API-->>Client: Document ID + Status
    
    Note over Client,Safe: Q&A Query Flow
    Client->>API: POST /chat/query
    API->>LS: RAG Query Request
    LS->>Emb: Query Embedding
    Emb-->>LS: Query Vector
    LS->>Vec: Similarity Search
    Vec-->>LS: Top-K Chunks
    LS->>Inf: Generate Response
    Inf-->>LS: Raw Response
    LS->>Safe: Safety Check
    Safe-->>LS: Filtered Response
    LS-->>API: Final Answer + Citations
    API-->>Client: Streamed Response
```

### **Provider Initialization Code**

```python
# llamastack/providers/granite_setup.py
from llama_stack.client import LlamaStackClient

async def initialize_llamastack():
    """Initialize LlamaStack with M4-optimized providers"""
    
    client = LlamaStackClient(
        base_url="http://localhost:5001",
        timeout=30.0
    )
    
    # Configure Inference Provider
    inference_config = {
        "provider_type": "meta-reference",
        "config": {
            "model": "meta-llama/Llama-3.3-8B-Instruct",
            "device": "mps",  # M4 Metal Performance Shaders
            "torch_dtype": "float16",
            "max_seq_len": 4096,
            "max_batch_size": 1
        }
    }
    
    # Configure Embedding Provider
    embedding_config = {
        "provider_type": "meta-reference",
        "config": {
            "model": "meta-llama/Llama-Guard-3-30M-Embedding",
            "device": "mps",
            "torch_dtype": "float16",
            "max_seq_len": 512
        }
    }
    
    # Configure Vector Store
    vector_config = {
        "provider_type": "sqlite-vec",
        "config": {
            "db_path": "./data/vectors/main.db",
            "embedding_dim": 384,
            "similarity_metric": "cosine"
        }
    }
    
    return client

# Usage in FastAPI app
async def create_llamastack_client():
    return await initialize_llamastack()
```

---

## ğŸ”„ **Data Flow & Processing**

### **Document Processing Pipeline**

```mermaid
flowchart TD
    START([ğŸ“¥ Document Upload]) --> DETECT{ğŸ” File Type Detection}
    
    DETECT -->|PDF| PDF[ğŸ“„ PDF Processor<br/>pdfplumber]
    DETECT -->|DOCX| DOCX[ğŸ“ DOCX Processor<br/>python-docx]
    DETECT -->|URL| URL[ğŸŒ URL Processor<br/>newspaper3k/trafilatura]
    DETECT -->|MD/TXT| TEXT[ğŸ“ Text Processor<br/>direct read]
    
    PDF --> CLEAN[ğŸ§¹ Text Cleaning<br/>Remove artifacts]
    DOCX --> CLEAN
    URL --> CLEAN
    TEXT --> CLEAN
    
    CLEAN --> CHUNK[âœ‚ï¸ Smart Chunking<br/>Semantic boundaries]
    CHUNK --> METADATA[ğŸ“Š Extract Metadata<br/>Title, author, date]
    
    METADATA --> EMBED[ğŸ§® Generate Embeddings<br/>Granite Embedding Model]
    EMBED --> STORE_VEC[ğŸ—„ï¸ Store Vectors<br/>SQLite-Vec Database]
    EMBED --> STORE_META[ğŸ“Š Store Metadata<br/>SQLite Database]
    
    STORE_VEC --> INDEX[ğŸ” Update Search Index]
    STORE_META --> INDEX
    INDEX --> COMPLETE([âœ… Processing Complete])
    
    style START fill:#e1f5fe
    style COMPLETE fill:#e8f5e8
    style EMBED fill:#fff3e0
    style STORE_VEC fill:#f3e5f5
```

### **RAG Query Processing**

```mermaid
flowchart TD
    QUERY([ğŸ’¬ User Query]) --> PREPROCESS[ğŸ”§ Query Preprocessing<br/>Clean & validate]
    
    PREPROCESS --> EMBED_Q[ğŸ§® Query Embedding<br/>Granite Embedding]
    EMBED_Q --> SEARCH[ğŸ” Vector Similarity Search<br/>SQLite-Vec cosine similarity]
    
    SEARCH --> FILTER{ğŸ¯ Relevance Filter<br/>Score > threshold}
    FILTER -->|Pass| RERANK[ğŸ“Š Rerank Results<br/>By relevance score]
    FILTER -->|Fail| FALLBACK[ğŸ¤– Fallback Response<br/>No relevant docs found]
    
    RERANK --> CONTEXT[ğŸ“ Build Context<br/>Top-K chunks + metadata]
    CONTEXT --> PROMPT[ğŸ“‹ Prompt Template<br/>System + context + query]
    
    PROMPT --> LLM[ğŸ¤– LLM Inference<br/>Granite-3.3-8B-Instruct]
    LLM --> SAFETY[ğŸ›¡ï¸ Safety Check<br/>Granite Guardian]
    
    SAFETY --> CITATIONS[ğŸ“š Extract Citations<br/>Map sources to response]
    CITATIONS --> RESPONSE[ğŸ“¤ Final Response<br/>Answer + citations]
    
    FALLBACK --> RESPONSE
    RESPONSE --> STREAM([ğŸ“¡ Stream to Client])
    
    style QUERY fill:#e1f5fe
    style STREAM fill:#e8f5e8
    style LLM fill:#fff3e0
    style SAFETY fill:#ffebee
```

---

## âœ¨ **Features**

### **ğŸ¯ Core Capabilities**
- **ğŸ“„ Multi-Format Document Support**: PDF, DOCX, TXT, Markdown, URLs
- **ğŸ§  Intelligent Q&A**: Contextual responses with source citations
- **ğŸ” Semantic Search**: Vector-based similarity search with reranking
- **ğŸ’¬ Real-time Chat**: Streaming responses with typing indicators
- **ğŸ“š Source Management**: Upload, view, organize, and delete documents
- **ğŸµ Audio Summaries**: Text-to-speech generation for document overviews
- **ğŸ›¡ï¸ Safety Guardrails**: Content filtering and response validation
- **âš¡ M4 Optimization**: Native Apple Silicon performance tuning

### **ğŸ”§ Technical Features**
- **ğŸ¦™ LlamaStack Integration**: Provider-based architecture
- **ğŸš€ Async Processing**: Background document processing
- **ğŸ’¾ Intelligent Caching**: Response and embedding caching
- **ğŸ“Š Performance Monitoring**: Real-time metrics and health checks
- **ğŸ³ Containerized Deployment**: Docker and docker-compose ready
- **ğŸ” Security**: API rate limiting and input validation

---

## ğŸš€ **Quick Start**

### **Prerequisites**
- Python 3.12+
- M4 MacBook (or compatible ARM64/x86_64)
- 8GB+ RAM recommended
- 10GB+ free disk space

### **1. Clone & Setup**
```bash
git clone https://github.com/abhijeet-dhumal/rag-notebooklm-llama-stack.git
cd rag-notebooklm-llama-stack

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
make install
```

### **2. Environment Configuration**
```bash
# Copy environment template
cp .env.example .env

# Edit configuration (optional)
nano .env
```

### **3. Start Services**
```bash
# Start LlamaStack server (Terminal 1)
make llamastack-start

# Start backend API (Terminal 2)
make dev

# Start frontend UI (Terminal 3)
make frontend
```

### **4. Access Application**
- **Frontend UI**: http://localhost:8501
- **Backend API**: http://localhost:8000
- **API Docs**: http://localhost:8000/docs
- **LlamaStack**: http://localhost:5001

---

## ğŸ› ï¸ **Development**

### **Project Structure**
```
rag-notebooklm-llama-stack/
â”œâ”€â”€ backend/                 # FastAPI backend
â”‚   â”œâ”€â”€ api/                # API endpoints
â”‚   â”œâ”€â”€ core/               # Business logic
â”‚   â”œâ”€â”€ models/             # Data models
â”‚   â””â”€â”€ config/             # Configuration
â”œâ”€â”€ frontend/               # Streamlit frontend
â”‚   â”œâ”€â”€ streamlit/          # Streamlit app
â”‚   â””â”€â”€ react/              # React app (future)
â”œâ”€â”€ llamastack/             # LlamaStack configuration
â”‚   â”œâ”€â”€ config/             # Provider configs
â”‚   â””â”€â”€ providers/          # Custom providers
â”œâ”€â”€ data/                   # Data storage
â”‚   â”œâ”€â”€ documents/          # Uploaded files
â”‚   â”œâ”€â”€ vectors/            # Vector database
â”‚   â””â”€â”€ models/             # Downloaded models
â””â”€â”€ docs/                   # Documentation
```

### **Development Commands**
```bash
# Code formatting
make format

# Run tests
make test

# Health check
make health

# Clean cache
make clean

# Docker build
make docker-build
```

### **Adding New Document Types**
1. Create processor in `backend/core/document_processor/`
2. Register in `backend/core/document_processor/__init__.py`
3. Add MIME type detection
4. Update API documentation

### **Extending LlamaStack Providers**
1. Implement provider in `llamastack/providers/`
2. Update configuration in `llamastack/config/`
3. Register provider in initialization

---

## ğŸ“š **API Documentation**

### **Key Endpoints**

| Method | Endpoint | Description | Request | Response |
|--------|----------|-------------|---------|----------|
| `POST` | `/api/v1/documents` | Upload document | `multipart/form-data` | `DocumentResponse` |
| `POST` | `/api/v1/documents/url` | Add from URL | `URLUpload` | `DocumentResponse` |
| `GET` | `/api/v1/documents` | List documents | Query params | `DocumentList` |
| `POST` | `/api/v1/chat/query` | Ask question | `ChatQuery` | `ChatResponse` |
| `GET` | `/api/v1/health` | Health check | None | `HealthStatus` |

### **Example Usage**

```python
import httpx

# Upload document
async with httpx.AsyncClient() as client:
    with open("document.pdf", "rb") as f:
        response = await client.post(
            "http://localhost:8000/api/v1/documents",
            files={"file": f},
            data={"title": "My Document"}
        )
    document = response.json()

# Ask question
query_response = await client.post(
    "http://localhost:8000/api/v1/chat/query",
    json={
        "query": "What are the main points?",
        "document_ids": [document["id"]]
    }
)
answer = query_response.json()
```

For complete API documentation, visit: http://localhost:8000/docs

---

## ğŸ† **Performance & Optimization**

### **M4 MacBook Optimizations**
- **ğŸ”¥ Metal Performance Shaders**: GPU acceleration for inference
- **âš¡ ARM64 Native**: Optimized libraries for Apple Silicon
- **ğŸ’¾ Unified Memory**: Efficient memory allocation
- **ğŸ§® Mixed Precision**: FP16 for faster inference

### **Performance Metrics**
- **Response Time**: <3 seconds for typical queries
- **Throughput**: 50+ concurrent users
- **Memory Usage**: <8GB total system memory
- **Storage**: Efficient vector compression

---

## ğŸ“– **Documentation**

This README contains complete project documentation including:

- **ğŸ›ï¸ System Architecture**: Complete architecture diagrams and component details
- **ğŸ¦™ LlamaStack Integration**: Provider configuration and usage patterns  
- **ğŸ”„ Data Flow**: Document processing and RAG query pipelines
- **ğŸš€ Quick Start**: Step-by-step setup and running instructions
- **ğŸ› ï¸ Development**: Project structure and development guidelines
- **ğŸ“š API Reference**: Key endpoints and usage examples

### **Additional Reference Files**
- **[API Specification](./api_specification.yaml)**: Complete OpenAPI 3.0 specification
- **[Sprint Plan](./3DAY_SPRINT_PLAN.md)**: Development roadmap and implementation timeline

---

## ğŸ¤ **Contributing**

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

---

## ğŸ“„ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ **Acknowledgments**

- **Meta AI**: LlamaStack framework and Granite models
- **FastAPI**: High-performance web framework
- **Streamlit**: Rapid UI development
- **SQLite-Vec**: Efficient vector storage

---

**ğŸš€ Happy RAG Building!** For questions and support, please open an issue on GitHub.
