# ğŸ¦™ **RAG LlamaStack - Streamlit Edition**

> **A modern, intelligent RAG application with real-time document processing**  
> Built with LlamaStack orchestration and Streamlit for seamless AI interactions

## âš¡ **30-Second Start**

```bash
git clone https://github.com/yourusername/rag-llama-stack.git
cd rag-llama-stack
python -m venv venv && source venv/bin/activate
make setup && make start
# Open: http://localhost:8501
```

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io)
[![LlamaStack](https://img.shields.io/badge/LlamaStack-Latest-green.svg)](https://github.com/meta-llama/llama-stack)

---

## ğŸ“‹ **Table of Contents**

1. [ğŸ¯ Features](#-features)
2. [ğŸ›ï¸ System Architecture](#-system-architecture)
3. [ğŸ”„ Data Flow & Processing](#-data-flow--processing)
4. [ğŸš€ Quick Start](#-quick-start)
5. [ğŸ“Š Performance & Monitoring](#-performance--monitoring)
6. [ğŸ”§ Configuration](#-configuration)
7. [ğŸ› Troubleshooting](#-troubleshooting)

---

## ğŸ¯ **Features**

### ğŸ”¥ **Core Capabilities**
- **ğŸ“„ Multi-format Document Processing** - PDF, DOCX, PPTX, TXT, MD (up to 50MB)
- **ğŸ¤– Intelligent Q&A** - Context-aware responses with source citations
- **ğŸ” Semantic Search** - Advanced embedding-based document retrieval
- **âš¡ Real-time Processing** - Live progress tracking and performance metrics
- **ğŸ¨ Modern UI** - Dark/light theme with responsive design

### ğŸ› ï¸ **Advanced Features**
- **ğŸ“Š System Status Monitoring** - Real-time LlamaStack and Ollama health checks
- **ğŸ©º Connection Diagnostics** - Smart endpoint detection and troubleshooting
- **ğŸ“ˆ Performance Analytics** - Detailed processing metrics and statistics
- **ğŸ”„ Upload State Management** - Interrupt-resistant file processing
- **ğŸ” Debug Tools** - Comprehensive system diagnostics and logging

### ğŸ§  **AI Integration**
- **ğŸ¦™ LlamaStack Orchestration** - Unified API for inference and embeddings
- **ğŸ  Local Model Support** - Ollama integration for privacy-focused AI
- **ğŸ§® Sentence Transformers** - High-quality embeddings with all-MiniLM-L6-v2
- **ğŸ”€ Fallback Systems** - Multiple AI provider support with auto-switching

---

## ğŸ›ï¸ **System Architecture**

### **Current Streamlit-Only Architecture**

```mermaid
graph TB
    subgraph "ğŸ¨ Frontend Layer (Streamlit)"
        UI[ğŸ“± Main Interface]
        SIDEBAR[ğŸ”§ Control Sidebar]
        CHAT[ğŸ’¬ Chat Interface]
        UPLOAD[ğŸ“ File Upload]
        STATUS[ğŸ”Œ System Status]
        DIAG[ğŸ©º Diagnostics]
    end
    
    subgraph "ğŸ§  Core Processing"
        DOC_HANDLER[ğŸ“„ Document Handler]
        CHAT_ENGINE[ğŸ’¬ Chat Engine]
        MODEL_MGR[ğŸ¤– Model Manager]
        THEME_MGR[ğŸ¨ Theme Manager]
    end
    
    subgraph "ğŸ¦™ LlamaStack Integration"
        LS_CLIENT[ğŸ”— LlamaStack Client]
        EMBED_API[ğŸ§® Embeddings API]
        CHAT_API[ğŸ’¬ Chat Completion API]
        HEALTH_API[ğŸ’“ Health Check API]
    end
    
    subgraph "ğŸ  Local AI (Ollama)"
        OLLAMA[ğŸ¦™ Ollama Server]
        LOCAL_LLM[ğŸ¤– Local LLM Models]
        MODEL_PULL[â¬‡ï¸ Model Management]
    end
    
    subgraph "ğŸ’¾ Storage & State"
        SESSION[ğŸ”„ Session State]
        VECTOR_DB[ğŸ—„ï¸ Vector Storage]
        DOC_STORE[ğŸ“Š Document Storage]
        CACHE[âš¡ Performance Cache]
    end
    
    %% User Interactions
    UI --> SIDEBAR
    SIDEBAR --> STATUS
    SIDEBAR --> UPLOAD
    SIDEBAR --> DIAG
    UI --> CHAT
    
    %% Core Processing Flow
    UPLOAD --> DOC_HANDLER
    CHAT --> CHAT_ENGINE
    STATUS --> MODEL_MGR
    DIAG --> MODEL_MGR
    
    %% LlamaStack Integration
    DOC_HANDLER --> LS_CLIENT
    CHAT_ENGINE --> LS_CLIENT
    MODEL_MGR --> LS_CLIENT
    LS_CLIENT --> EMBED_API
    LS_CLIENT --> CHAT_API
    LS_CLIENT --> HEALTH_API
    
    %% Ollama Fallback
    LS_CLIENT -.->|Fallback| OLLAMA
    MODEL_MGR --> OLLAMA
    OLLAMA --> LOCAL_LLM
    OLLAMA --> MODEL_PULL
    
    %% Storage Layer
    DOC_HANDLER --> SESSION
    CHAT_ENGINE --> SESSION
    SESSION --> VECTOR_DB
    SESSION --> DOC_STORE
    SESSION --> CACHE
    
    style UI fill:#e3f2fd
    style LS_CLIENT fill:#fff3e0
    style OLLAMA fill:#e8f5e8
    style SESSION fill:#fce4ec
```

### **Technology Stack**

```mermaid
graph LR
    subgraph "ğŸ–¥ï¸ Frontend"
        ST[Streamlit 1.28+]
        CSS[Custom CSS/JS]
        PD[Pandas DataFrames]
    end
    
    subgraph "ğŸ¤– AI/ML"
        LS[LlamaStack API]
        ST_EMB[Sentence Transformers]
        OL[Ollama]
        HF[Hugging Face]
    end
    
    subgraph "ğŸ“Š Data Processing"
        DOC[Docling]
        PDF[PyPDF2]
        NP[NumPy]
        JSON[JSON Storage]
    end
    
    subgraph "ğŸ”§ Infrastructure"
        PY[Python 3.12+]
        REQ[Requests]
        SUB[Subprocess]
        OS[OS Integration]
    end
    
    ST --> LS
    ST --> CSS
    ST --> PD
    LS --> ST_EMB
    LS --> OL
    DOC --> PDF
    DOC --> NP
    ST --> DOC
```

---

## ğŸ”„ **Data Flow & Processing**

### **Document Processing Pipeline**

```mermaid
flowchart TD
    START([ğŸ“ File Upload]) --> VALIDATE{ğŸ” Validation}
    
    VALIDATE -->|âœ… Valid| STATE_TRACK[ğŸ”„ State Tracking<br/>Mark as uploading]
    VALIDATE -->|âŒ Invalid| ERROR[âŒ Error Display<br/>Size/type limits]
    
    STATE_TRACK --> EXTRACT[ğŸ“„ Content Extraction<br/>Multi-format support]
    EXTRACT --> OPTIMIZE[ğŸš€ Performance Optimization<br/>Large file handling]
    
    OPTIMIZE --> CHUNK[âœ‚ï¸ Smart Chunking<br/>3000 chars + 600 overlap]
    CHUNK --> BATCH[ğŸ“¦ Batch Processing<br/>Optimized embedding generation]
    
    BATCH --> EMBED[ğŸ§® Generate Embeddings<br/>all-MiniLM-L6-v2]
    EMBED --> QUALITY[ğŸ¯ Quality Check<br/>Validate embeddings]
    
    QUALITY -->|âœ… Success| STORE[ğŸ’¾ Store Document<br/>Session state + backup]
    QUALITY -->|âš ï¸ Partial| FALLBACK[ğŸ§ª Dummy Embeddings<br/>Ensure functionality]
    
    STORE --> METRICS[ğŸ“Š Performance Metrics<br/>Speed, quality, stats]
    FALLBACK --> METRICS
    METRICS --> COMPLETE([âœ… Processing Complete])
    
    STATE_TRACK -.->|Interruption| RETRY[ğŸ”„ Mark for Retry<br/>State management]
    RETRY -.-> STATE_TRACK
    
    style START fill:#e1f5fe
    style COMPLETE fill:#e8f5e8
    style EMBED fill:#fff3e0
    style METRICS fill:#f3e5f5
    style ERROR fill:#ffebee
```

### **Chat & Query Processing**

```mermaid
flowchart TD
    QUERY([ğŸ’¬ User Question]) --> CHECK{ğŸ“Š Documents Available?}
    
    CHECK -->|âŒ No| NO_DOCS[ğŸ“ No Documents Message<br/>Upload prompt]
    CHECK -->|âœ… Yes| EMBED_Q[ğŸ§® Query Embedding<br/>all-MiniLM-L6-v2]
    
    EMBED_Q --> SEARCH[ğŸ” Similarity Search<br/>Cosine similarity]
    SEARCH --> FILTER[ğŸ¯ Relevance Filtering<br/>Threshold: 0.25]
    
    FILTER --> RERANK[ğŸ“Š Chunk Reranking<br/>Diversity + relevance]
    RERANK --> CONTEXT[ğŸ“ Context Building<br/>6000 char limit]
    
    CONTEXT --> PROMPT[ğŸ“‹ Prompt Engineering<br/>System + context + query]
    PROMPT --> TRY_LS[ğŸ¦™ Try LlamaStack<br/>Multiple endpoints]
    
    TRY_LS -->|âœ… Success| RESPONSE[ğŸ“¤ AI Response<br/>With citations]
    TRY_LS -->|âŒ Failed| TRY_OLLAMA[ğŸ  Try Ollama Fallback<br/>Local processing]
    
    TRY_OLLAMA -->|âœ… Success| RESPONSE
    TRY_OLLAMA -->|âŒ Failed| FALLBACK_RESP[ğŸ¤– Structured Fallback<br/>Context-based response]
    
    RESPONSE --> SOURCES[ğŸ“š Extract Sources<br/>Top 3 documents]
    FALLBACK_RESP --> SOURCES
    SOURCES --> DISPLAY[ğŸ“± Display Response<br/>Chat interface]
    
    NO_DOCS --> DISPLAY
    
    style QUERY fill:#e3f2fd
    style RESPONSE fill:#e8f5e8
    style FALLBACK_RESP fill:#fff3e0
    style NO_DOCS fill:#ffebee
```

### **System Health & Diagnostics**

```mermaid
flowchart TD
    MONITOR([ğŸ”Œ System Monitor]) --> CHECK_LS[ğŸ¦™ Check LlamaStack<br/>Health endpoint]
    MONITOR --> CHECK_OL[ğŸ  Check Ollama<br/>Model list]
    
    CHECK_LS -->|âœ… Online| LS_DIAG[ğŸ©º LlamaStack Diagnostics<br/>Endpoint discovery]
    CHECK_LS -->|âŒ Offline| LS_ERROR[âŒ Connection Issues<br/>Show recommendations]
    
    CHECK_OL -->|âœ… Running| OL_MODELS[ğŸ“¦ List Models<br/>Available models]
    CHECK_OL -->|âŒ Offline| OL_ERROR[âŒ Ollama Down<br/>Installation guide]
    
    LS_DIAG --> TEST_ENDPOINTS[ğŸ“¡ Test Endpoints<br/>Models, chat, embeddings]
    TEST_ENDPOINTS --> RECOMMEND[ğŸ’¡ Recommendations<br/>Fix suggestions]
    
    OL_MODELS --> MODEL_STATUS[ğŸ“Š Model Status<br/>Local vs remote]
    
    RECOMMEND --> STATUS_UI[ğŸ“± Status Display<br/>Real-time indicators]
    MODEL_STATUS --> STATUS_UI
    LS_ERROR --> STATUS_UI
    OL_ERROR --> STATUS_UI
    
    style MONITOR fill:#e3f2fd
    style STATUS_UI fill:#e8f5e8
    style LS_ERROR fill:#ffebee
    style OL_ERROR fill:#ffebee
```

---

## ğŸš€ **Quick Start**

### **Prerequisites**
- **Python 3.12+** (recommended)
- **Git** for cloning
- **8GB+ RAM** for local models
- **Optional**: Ollama for local AI processing

### **Installation & Setup**

```bash
# 1. Clone the repository
git clone https://github.com/yourusername/rag-llama-stack.git
cd rag-llama-stack

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Setup LlamaStack (automated)
make setup

# 5. Start the application
make start
```

### **Alternative: Manual Setup**

```bash
# Start LlamaStack server
llamastack run ./llamastack/config/llamastack-config.yaml

# In another terminal, start Streamlit
streamlit run frontend/streamlit/app.py --server.port 8501
```

### **First Time Usage**

1. **Open** http://localhost:8501
2. **Check System Status** in the top-left sidebar
3. **Upload Documents** using the file uploader
4. **Start Chatting** with your documents!

---

## ğŸ“Š **Performance & Monitoring**

### **Real-time System Status**

The application provides comprehensive monitoring:

- **ğŸŸ¢ LlamaStack** - Connection and endpoint health
- **ğŸŸ¢ Ollama** - Local model availability  
- **ğŸ“Š Performance Metrics** - Processing speed and quality
- **ğŸ” Debug Information** - Configuration and state details

### **Document Processing Metrics**

Each upload provides detailed analytics:

| Metric | Description | Typical Range |
|--------|-------------|---------------|
| **Processing Speed** | MB/second throughput | 0.2-1.0 MB/s |
| **Embedding Quality** | Success rate percentage | 95-100% |
| **Chunk Efficiency** | Characters per chunk | 2500-3500 |
| **Memory Usage** | Session state size | <50MB |

### **Performance Optimization**

The system automatically optimizes for:
- **Large Files**: Batch processing and content filtering
- **Slow Networks**: Fallback systems and local processing
- **Memory**: Efficient chunk management and cleanup
- **Speed**: Parallel operations and smart caching

---

## ğŸ”§ **Configuration**

### **Main Configuration** (`frontend/streamlit/core/config.py`)

```python
# Model Configuration
DEFAULT_EMBEDDING_MODEL = "all-MiniLM-L6-v2"
DEFAULT_LLM_MODEL = "llama3.2:1b"

# Processing Configuration  
CHARS_PER_CHUNK = 3000
CHUNK_OVERLAP = 600
MAX_RELEVANT_CHUNKS = 4

# Performance Configuration
MIN_SIMILARITY_THRESHOLD = 0.25
LLM_TEMPERATURE = 0.4
LLM_MAX_TOKENS = 1024
```

### **Streamlit Configuration** (`.streamlit/config.toml`)

```toml
[server]
maxUploadSize = 50
port = 8501

[theme]
primaryColor = "#667eea"
backgroundColor = "#ffffff"

[browser]
gatherUsageStats = false
```

### **LlamaStack Configuration** (`llamastack/config/llamastack-config.yaml`)

```yaml
built_at: '2024-12-XX'
image_type: conda

apis:
  - inference
  - safety  
  - agents
  - memory
  - telemetry

providers:
  inference:
    - provider_id: ollama
      provider_type: remote::ollama
      config:
        url: http://localhost:11434
```

---

## ğŸ› **Troubleshooting**

### **Common Issues & Solutions**

#### ğŸ”´ **LlamaStack Connection Failed**
```bash
# Check if LlamaStack is running
curl http://localhost:8321/v1/health

# Restart LlamaStack
make restart

# Check configuration
cat llamastack/config/llamastack-config.yaml
```

#### ğŸ”´ **Ollama Not Found**
```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service
ollama serve

# Pull a model
ollama pull llama3.2:1b
```

#### ğŸ”´ **File Upload Fails**
- Check file size (max 50MB)
- Verify file format (PDF, DOCX, PPTX, TXT, MD)
- Don't switch models during upload
- Use "Retry" if interrupted

#### ğŸ”´ **Poor Response Quality**
- Upload more relevant documents
- Check embedding quality in performance metrics
- Verify model configuration
- Use connection diagnostics

### **Debug Mode**

Enable detailed logging:

```bash
# Set debug environment
export STREAMLIT_LOGGER_LEVEL=debug

# Run with verbose output
streamlit run frontend/streamlit/app.py --logger.level debug
```

### **Getting Help**

1. **Connection Diagnostics** - Use the built-in diagnostic tools
2. **Performance Metrics** - Check the detailed performance tables
3. **Debug Information** - Use the debug panel in the sidebar
4. **Logs** - Check `logs/` directory for detailed error logs

---

## ğŸ“œ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ **Contributing**

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## â­ **Star History**

If you find this project useful, please consider giving it a star! â­

---

*Built with â¤ï¸ using LlamaStack, Streamlit, and modern AI technologies*
