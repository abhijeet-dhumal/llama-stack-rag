"""
RAG LlamaStack Streamlit Application
Main application entry point - now modular and clean!
"""

import streamlit as st

# Import core modules
from core.config import PAGE_CONFIG, LLAMASTACK_BASE, APP_DESCRIPTION
from core.theme import initialize_theme, apply_theme, render_header_theme_toggle
from core.utils import (
    initialize_session_state, validate_llamastack_connection,
    cleanup_interrupted_uploads, process_uploaded_files_with_state_tracking
)
from core.document_handler import (
    initialize_document_storage, render_file_uploader, validate_uploaded_files,
    render_document_library, has_documents
)
from core.model_manager import render_model_dashboard, render_ollama_integration, get_model_info
from core.chat_interface import render_welcome_screen, render_chat_interface


def main():
    """Main application function"""
    # Configure page with menu items
    st.set_page_config(
        **PAGE_CONFIG,
        menu_items={
            'Get Help': None,
            'Report a bug': None,
            'About': """
            # ğŸ¦™ RAG LlamaStack
            
            **Retrieval-Augmented Generation with LlamaStack & Ollama**
            
            ### Features:
            - ğŸ“„ Document processing (PDF, TXT, MD, DOCX, PPTX)
            - ğŸ” Semantic search with embeddings
            - ğŸ¤– Local LLM inference with Ollama
            - ğŸ¨ Dark/Light theme support
            - ğŸ“Š Real-time processing metrics
            
            ### Settings & Options:
            - **Theme Toggle:** Use the â˜€ï¸/ğŸŒ™ button in top-right header
            - **Model Selection:** Configure in left sidebar
            - **File Upload:** Max 50MB per file, multiple formats supported
            - **Dashboard & Settings:** Available in this 3-dot menu
            
            ### Tech Stack:
            Built with Streamlit + LlamaStack + Ollama for local AI processing
            """
        }
    )
    
    # Initialize all systems
    initialize_theme()
    initialize_session_state()
    initialize_document_storage()
    
    # Apply theme
    apply_theme()
    
    # Check LlamaStack connection
    if not validate_llamastack_connection():
        st.error("ğŸ”´ LlamaStack Offline")
        st.stop()
    
    # Header (no top menu bar)
    render_header()
    
    # Sidebar
    render_sidebar()
    
    # Main content area
    if not has_documents():
        render_welcome_screen()
    else:
        render_chat_interface()


def render_header():
    """Render clean, minimal header with branding"""
    # Render the fixed-position theme toggle
    render_header_theme_toggle()
    
    # Simple header with just branding
    st.title("ğŸ¦™ RAG LlamaStack")
    st.caption("Retrieval-Augmented Generation with LlamaStack & Ollama")


def render_sidebar():
    """Render the sidebar with all components"""
    with st.sidebar:
        # System Status Section (Top Priority)
        st.markdown("### ğŸ”Œ System Status")
        
        # Connection status with real-time indicators
        col1, col2 = st.columns(2)
        with col1:
            if validate_llamastack_connection():
                st.success("ğŸŸ¢ LlamaStack")
            else:
                st.error("ğŸ”´ LlamaStack")
        
        with col2:
            # Check Ollama status
            try:
                import subprocess
                result = subprocess.run(["ollama", "list"], capture_output=True, text=True, timeout=3)
                if result.returncode == 0:
                    st.success("ğŸŸ¢ Ollama")
                else:
                    st.warning("ğŸŸ¡ Ollama")
            except Exception:
                st.error("ğŸ”´ Ollama")
        
        # Quick setup info (collapsible)
        with st.expander("âš™ï¸ Configuration & Help", expanded=False):
            st.markdown("""
            **Current Setup:**
            - ğŸ” Embeddings: all-MiniLM-L6-v2 (sentence-transformers)
            - ğŸ§  LLM: See Model Dashboard below
            
            **Connection Issues?**
            - Use diagnostics in Model Dashboard
            - Check `llamastack-config.yaml`
            - Restart with `make restart`
            
            **Performance Tips:**
            - Add API providers for faster responses
            - Use local Ollama models for privacy
            """)
        
        st.markdown("---")
        
        # Model Dashboard
        render_model_dashboard()
        
        # Document Upload Section
        uploaded_files = render_file_uploader()
        
        if uploaded_files:
            valid_files = validate_uploaded_files(uploaded_files)
            if valid_files:
                # Clean up any previous interrupted state
                cleanup_interrupted_uploads()
                
                # Check for new files that haven't been successfully processed
                new_files = []
                retry_files = []
                
                for file in valid_files:
                    file_id = f"{file.name}_{file.size}"
                    
                    # Check if file is in failed uploads (needs retry)
                    if file_id in st.session_state.failed_uploads:
                        retry_files.append(file)
                        st.session_state.failed_uploads.discard(file_id)
                        continue
                    
                    # Check if file is currently being uploaded
                    if file_id in st.session_state.currently_uploading:
                        st.warning(f"â³ File `{file.name}` is currently being processed...")
                        continue
                    
                    # Check if file is already successfully processed
                    if file_id not in st.session_state.processed_files:
                        new_files.append(file)
                
                # Show retry message if applicable
                if retry_files:
                    st.info(f"ğŸ”„ Retrying {len(retry_files)} previously failed upload(s)")
                
                # Process new and retry files
                files_to_process = new_files + retry_files
                if files_to_process:
                    process_uploaded_files_with_state_tracking(files_to_process)
        
        # Document Library
        render_document_library()
        
        # Ollama Integration
        render_ollama_integration()
        
        # Connection Diagnostics & Debug Tools (Bottom of navbar)
        st.markdown("---")
        with st.expander("ğŸ” Connection Diagnostics & Debug", expanded=False):
            st.markdown("**Advanced troubleshooting tools**")
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("ğŸ©º Diagnose LlamaStack", help="Check LlamaStack endpoints and connectivity"):
                    with st.spinner("Diagnosing LlamaStack..."):
                        diagnosis = st.session_state.llamastack_client.diagnose_llamastack()
                        
                        if diagnosis["reachable"]:
                            st.success(f"âœ… LlamaStack reachable at {diagnosis['base_url']}")
                            
                            if diagnosis["available_endpoints"]:
                                st.info("ğŸ“¡ **Available Endpoints:**")
                                for endpoint in diagnosis["available_endpoints"]:
                                    st.text(f"â€¢ {endpoint}")
                            else:
                                st.warning("âš ï¸ No API endpoints found")
                        else:
                            st.error(f"âŒ Cannot reach LlamaStack at {diagnosis['base_url']}")
                        
                        if diagnosis["recommendations"]:
                            st.warning("ğŸ’¡ **Recommendations:**")
                            for rec in diagnosis["recommendations"]:
                                st.text(f"â€¢ {rec}")
            
            with col2:
                if st.button("ğŸ¦™ Check Ollama", help="Check if Ollama is running and has models"):
                    with st.spinner("Checking Ollama..."):
                        from core.model_manager import check_ollama_status
                        ollama_status = check_ollama_status()
                        
                        if ollama_status["running"]:
                            st.success("âœ… Ollama is running")
                            if ollama_status["models"]:
                                st.info(f"ğŸ“¦ **{len(ollama_status['models'])} models available:**")
                                for model in ollama_status["models"][:5]:  # Show first 5
                                    st.text(f"â€¢ {model['name']}")
                                if len(ollama_status["models"]) > 5:
                                    st.text(f"... and {len(ollama_status['models']) - 5} more")
                            else:
                                st.warning("âš ï¸ No models found in Ollama")
                                st.info("ğŸ’¡ Pull a model with: `ollama pull llama3.2:1b`")
                        else:
                            st.error("âŒ Ollama is not running")
                            st.info("ğŸ’¡ Start Ollama with: `ollama serve`")
            
            # Debug information
            st.markdown("---")
            st.markdown("**ğŸ› Debug Information**")
            
            if st.button("ğŸ“‹ Show Debug Info", help="Display current configuration and state"):
                debug_info = {
                    "LlamaStack URL": st.session_state.llamastack_client.base_url,
                    "Selected LLM": st.session_state.selected_llm_model,
                    "Selected Embedding": st.session_state.selected_embedding_model,
                    "Documents Loaded": len(st.session_state.uploaded_documents) if 'uploaded_documents' in st.session_state else 0,
                    "Chat History": len(st.session_state.chat_history) if 'chat_history' in st.session_state else 0
                }
                
                st.json(debug_info)


if __name__ == "__main__":
    main()
